https://github.com/elastic/elasticsearch/pull/114819

I index locally a few indices with a keyword field holding 100 million unique values and then execute the following query:

FROM test_index* | STATS x = count() BY keyword_high | SORT x DESC | LIMIT 10
This error out with the following stacktrace:

java.lang.IllegalArgumentException: BytesStreamOutput cannot hold more than 2GB of data
org.elasticsearch.server@9.0.0-SNAPSHOT/org.elasticsearch.common.io.stream.BytesStreamOutput.ensureCapacity(BytesStreamOutput.java:174)
org.elasticsearch.server@9.0.0-SNAPSHOT/org.elasticsearch.common.io.stream.BytesStreamOutput.writeBytes(BytesStreamOutput.java:85)
org.elasticsearch.server@9.0.0-SNAPSHOT/org.elasticsearch.common.util.BytesRefArray.writeTo(BytesRefArray.java:159)
org.elasticsearch.compute.aggregation.blockhash.BytesRefBlockHash.getKeys(BytesRefBlockHash.java:189)
org.elasticsearch.compute.aggregation.blockhash.BytesRefBlockHash.getKeys(BytesRefBlockHash.java:36)
org.elasticsearch.compute.operator.HashAggregationOperator.finish(HashAggregationOperator.java:194)
org.elasticsearch.compute.operator.Driver.runSingleLoopIteration(Driver.java:272)
org.elasticsearch.compute.operator.Driver.run(Driver.java:189)
org.elasticsearch.compute.operator.Driver$1.doRun(Driver.java:378)
The issue is here where we are trying to copy too much data into the BytesStreamOutput.


/*
 * Copyright Elasticsearch B.V. and/or licensed to Elasticsearch B.V. under one
 * or more contributor license agreements. Licensed under the Elastic License
 * 2.0; you may not use this file except in compliance with the Elastic License
 * 2.0.
 */

package org.elasticsearch.compute.aggregation.blockhash;

import org.apache.lucene.util.BytesRef;
import org.elasticsearch.common.io.stream.BytesStreamOutput;
import org.elasticsearch.common.io.stream.StreamInput;
import org.elasticsearch.common.unit.ByteSizeValue;
import org.elasticsearch.common.util.BigArrays;
import org.elasticsearch.common.util.BitArray;
import org.elasticsearch.common.util.BytesRefArray;
import org.elasticsearch.common.util.BytesRefHash;
import org.elasticsearch.compute.aggregation.GroupingAggregatorFunction;
import org.elasticsearch.compute.aggregation.SeenGroupIds;
import org.elasticsearch.compute.data.BlockFactory;
import org.elasticsearch.compute.data.BytesRefBlock;
import org.elasticsearch.compute.data.BytesRefVector;
import org.elasticsearch.compute.data.IntBlock;
import org.elasticsearch.compute.data.IntVector;
import org.elasticsearch.compute.data.OrdinalBytesRefBlock;
import org.elasticsearch.compute.data.Page;
import org.elasticsearch.compute.operator.mvdedupe.MultivalueDedupe;
import org.elasticsearch.compute.operator.mvdedupe.MultivalueDedupeBytesRef;
import org.elasticsearch.core.ReleasableIterator;

import java.io.IOException;

/**
 * Maps a {@link BytesRefBlock} column to group ids.
 */
final class BytesRefBlockHash extends BlockHash {
    private final int channel;
    final BytesRefHash hash;

    /**
     * Have we seen any {@code null} values?
     * <p>
     *     We reserve the 0 ordinal for the {@code null} key so methods like
     *     {@link #nonEmpty} need to skip 0 if we haven't seen any null values.
     * </p>
     */
    private boolean seenNull;

    BytesRefBlockHash(int channel, BlockFactory blockFactory) {
        super(blockFactory);
        this.channel = channel;
        this.hash = new BytesRefHash(1, blockFactory.bigArrays());
    }

    @Override
    public void add(Page page, GroupingAggregatorFunction.AddInput addInput) {
        var block = page.getBlock(channel);
        if (block.areAllValuesNull()) {
            seenNull = true;
            try (IntVector groupIds = blockFactory.newConstantIntVector(0, block.getPositionCount())) {
                addInput.add(0, groupIds);
            }
            return;
        }
        BytesRefBlock castBlock = (BytesRefBlock) block;
        BytesRefVector vector = castBlock.asVector();
        if (vector == null) {
            try (IntBlock groupIds = add(castBlock)) {
                addInput.add(0, groupIds);
            }
            return;
        }
        try (IntVector groupIds = add(vector)) {
            addInput.add(0, groupIds);
        }
    }

    IntVector add(BytesRefVector vector) {
        BytesRef scratch = new BytesRef();
        int positions = vector.getPositionCount();
        try (var builder = blockFactory.newIntVectorFixedBuilder(positions)) {
            for (int i = 0; i < positions; i++) {
                BytesRef v = vector.getBytesRef(i, scratch);
                builder.appendInt(Math.toIntExact(hashOrdToGroupNullReserved(hash.add(v))));
            }
            return builder.build();
        }
    }

    IntBlock add(BytesRefBlock block) {
        var ordinals = block.asOrdinals();
        if (ordinals != null) {
            return addOrdinalsBlock(ordinals);
        }
        MultivalueDedupe.HashResult result = new MultivalueDedupeBytesRef(block).hashAdd(blockFactory, hash);
        seenNull |= result.sawNull();
        return result.ords();
    }

    @Override
    public ReleasableIterator<IntBlock> lookup(Page page, ByteSizeValue targetBlockSize) {
        var block = page.getBlock(channel);
        if (block.areAllValuesNull()) {
            return ReleasableIterator.single(blockFactory.newConstantIntVector(0, block.getPositionCount()).asBlock());
        }

        BytesRefBlock castBlock = (BytesRefBlock) block;
        BytesRefVector vector = castBlock.asVector();
        // TODO honor targetBlockSize and chunk the pages if requested.
        if (vector == null) {
            return ReleasableIterator.single(lookup(castBlock));
        }
        return ReleasableIterator.single(lookup(vector));
    }

    private IntBlock addOrdinalsBlock(OrdinalBytesRefBlock inputBlock) {
        var inputOrds = inputBlock.getOrdinalsBlock();
        try (
            var builder = blockFactory.newIntBlockBuilder(inputOrds.getPositionCount());
            var hashOrds = add(inputBlock.getDictionaryVector())
        ) {
            for (int i = 0; i < inputOrds.getPositionCount(); i++) {
                int valueCount = inputOrds.getValueCount(i);
                int firstIndex = inputOrds.getFirstValueIndex(i);
                switch (valueCount) {
                    case 0 -> {
                        builder.appendInt(0);
                        seenNull = true;
                    }
                    case 1 -> {
                        int ord = hashOrds.getInt(inputOrds.getInt(firstIndex));
                        builder.appendInt(ord);
                    }
                    default -> {
                        builder.beginPositionEntry();
                        for (int v = 0; v < valueCount; v++) {
                            int ord = hashOrds.getInt(inputOrds.getInt(firstIndex + i));
                            builder.appendInt(ord);
                        }
                        builder.endPositionEntry();
                    }
                }
            }
            return builder.build();
        }
    }

    private IntBlock lookup(BytesRefVector vector) {
        BytesRef scratch = new BytesRef();
        int positions = vector.getPositionCount();
        try (var builder = blockFactory.newIntBlockBuilder(positions)) {
            for (int i = 0; i < positions; i++) {
                BytesRef v = vector.getBytesRef(i, scratch);
                long found = hash.find(v);
                if (found < 0) {
                    builder.appendNull();
                } else {
                    builder.appendInt(Math.toIntExact(hashOrdToGroupNullReserved(found)));
                }
            }
            return builder.build();
        }
    }

    private IntBlock lookup(BytesRefBlock block) {
        return new MultivalueDedupeBytesRef(block).hashLookup(blockFactory, hash);
    }

    @Override
    public BytesRefBlock[] getKeys() {
        /*
         * Create an un-owned copy of the data so we can close our BytesRefHash
         * without and still read from the block.
         */
        // TODO replace with takeBytesRefsOwnership ?!
        if (seenNull) {
            try (var builder = blockFactory.newBytesRefBlockBuilder(Math.toIntExact(hash.size() + 1))) {
                builder.appendNull();
                BytesRef spare = new BytesRef();
                for (long i = 0; i < hash.size(); i++) {
                    builder.appendBytesRef(hash.get(i, spare));
                }
                return new BytesRefBlock[] { builder.build() };
            }
        }

        final int size = Math.toIntExact(hash.size());
        try (BytesStreamOutput out = new BytesStreamOutput()) {
            hash.getBytesRefs().writeTo(out);
            try (StreamInput in = out.bytes().streamInput()) {
                return new BytesRefBlock[] {
                    blockFactory.newBytesRefArrayVector(new BytesRefArray(in, BigArrays.NON_RECYCLING_INSTANCE), size).asBlock() };
            }
        } catch (IOException e) {
            throw new IllegalStateException(e);
        }
    }

    @Override
    public IntVector nonEmpty() {
        return IntVector.range(seenNull ? 0 : 1, Math.toIntExact(hash.size() + 1), blockFactory);
    }

    @Override
    public BitArray seenGroupIds(BigArrays bigArrays) {
        return new SeenGroupIds.Range(seenNull ? 0 : 1, Math.toIntExact(hash.size() + 1)).seenGroupIds(bigArrays);
    }

    @Override
    public void close() {
        hash.close();
    }

    @Override
    public String toString() {
        StringBuilder b = new StringBuilder();
        b.append("BytesRefBlockHash{channel=").append(channel);
        b.append(", entries=").append(hash.size());
        b.append(", size=").append(ByteSizeValue.ofBytes(hash.ramBytesUsed()));
        b.append(", seenNull=").append(seenNull);
        return b.append('}').toString();
    }
}
ChatGPT said:
ChatGPT
The error you're encountering, java.lang.IllegalArgumentException: BytesStreamOutput cannot hold more than 2GB of data, indicates that the data being processed exceeds the maximum limit that BytesStreamOutput can handle. This is likely occurring during the aggregation operation where you're counting unique keywords in your indices.

Here are some suggestions to help resolve this issue:

Reduce Cardinality: If possible, try to reduce the cardinality of the keyword_high field. This could involve aggregating some of the values or filtering out less important ones before performing the stats aggregation.

Paginate or Chunk Data: Instead of trying to process all 100 million unique values at once, consider breaking down the data into smaller chunks. This can help avoid hitting the 2GB limit during aggregation.

Increase Memory Settings: If you haven't done so, consider increasing the heap size for your Elasticsearch node. This may provide more memory to handle larger datasets, although it won't specifically solve the BytesStreamOutput limit.

Use Different Aggregation Methods: Depending on your use case, explore using different aggregation methods that might be more memory efficient. For example, consider using terms aggregation with a size parameter to limit the number of unique keys being processed.

Update Elasticsearch Version: Ensure you're using a stable release version of Elasticsearch, as you're currently referencing a snapshot version (9.0.0-SNAPSHOT). Newer stable releases might have optimizations and fixes for issues like this.

Check for Memory Leaks: If you consistently hit this issue with large datasets, it might be worth investigating if there's a memory leak or inefficiency in how you're handling data. Monitor your cluster's memory usage closely.

Custom Implementation: If you're comfortable with Java and Elasticsearch internals, consider customizing how the aggregation is performed by implementing a more efficient version of the aggregation logic that avoids the need to collect all unique values into a single output stream.

These steps should help mitigate the issue. If the problem persists, you may want to engage with the Elasticsearch community or support for more specific guidance based on your use case and data patterns.



